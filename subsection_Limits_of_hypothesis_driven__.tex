\subsection{Limits of hypothesis-driven and knowledge creation via `expectation violation'}

Ironically learning from conceptual models often precedes hypothesis testing, precisely because the ideal of hypothesis-driven research fails in practice. Already before any hypothesis is tested, it is often found that models and simulations do not match the modellersâ€™ baseline expectations (or experimental data). It is in those moments that we start learning something truly new. Instead of testing a hypothesis, the models expose gaps in our knowledge and assumptions, enable us to generate new questions, or even enable us to suggest a novel way of conceptualizing a biological phenomenon. 

In effect, the original research question has been replaced with a more general one: why is the system (not) organised as we think it is, but differently? We can only look at differently organised systems, if we do not \emph{a priori} force a model to behave in one particular way. In other words, there is an important class of questions that can only be meaningfully answered by simply studying and interpreting the overal behaviour of a model, instead of narrowly focussing on a single hypothesis.